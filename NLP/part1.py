import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏—Ö –º–æ–≤–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å—ñ–≤
# nltk.download("punkt")  # –î–ª—è —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—ó
# nltk.download("stopwords")  # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
# nltk.download("wordnet")  # –î–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü—ñ—ó
# nltk.download("omw-1.4")  # WordNet –º–æ–≤–Ω—ñ –¥–∞–Ω—ñ

# üìò –í—Ö—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç
text = "Cats are chasing mice. The weather was beautiful, and we went out to play!"
# text = "Too small, would prefer a mat for a work surface that was anti static."

# --- 1. –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è ---
print("=== –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è ===")
sentences = sent_tokenize(text)
print("–†–µ—á–µ–Ω–Ω—è:", sentences)

words = word_tokenize(text)
print("–°–ª–æ–≤–∞:", words)

# --- 2. –°—Ç–æ–ø-—Å–ª–æ–≤–∞ ---
print("\n=== –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å—Ç–æ–ø-—Å–ª—ñ–≤ ===")
stop_words = set(stopwords.words("english"))
filtered_words = [
    word for word in words if word.lower() not in stop_words and word.isalpha()
]
print("–ë–µ–∑ —Å—Ç–æ–ø-—Å–ª—ñ–≤:", filtered_words)

# --- 3. –°—Ç–µ–º–º—ñ–Ω–≥ ---
print("\n=== –°—Ç–µ–º–º—ñ–Ω–≥ ===")
stemmer = PorterStemmer()
stemmed_words = [stemmer.stem(word) for word in filtered_words]
print("–ü—ñ—Å–ª—è —Å—Ç–µ–º–º—ñ–Ω–≥—É:", stemmed_words)

# --- 4. –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü—ñ—è ---
print("\n=== –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü—ñ—è ===")
lemmatizer = WordNetLemmatizer()
lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in filtered_words]
print("–ü—ñ—Å–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü—ñ—ó:", lemmatized_words)

nltk.download()
